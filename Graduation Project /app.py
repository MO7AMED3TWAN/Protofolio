import streamlit as st
import torch
import sys
import os
from pathlib import Path

# Add the project root to the Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Configuration
class RAGConfig:
    """Configuration for the RAG System"""
    # Paths
    DATABASE_PATH = os.path.join(project_root, "outputs/data/vector_database")
    
    # Model IDs
    EMBEDDING_MODEL = "Mo7amed3twan/GATE-AraBert-v2"
    LLM_MODEL = "Mo7amed3twan/GradModel-2B-RAG"
    
    # Model settings
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    MAX_LENGTH = 512
    TEMPERATURE = 0.5
    
    # Retrieval settings
    RETRIEVAL_K = 2
    CHUNK_SIZE = 1024

class ArabicChatBot:
    def __init__(self, rag_chain, vectorstore):
        self.rag_chain = rag_chain
        self.vectorstore = vectorstore

    def ask_question(self, question: str):
        if not self.rag_chain:
            return "Error Loading Rag System (Speak With Your Book)"

        try:
            response = self.rag_chain({"query": question})
            answer = response["result"]
            sources = response["source_documents"]

            return {
                "answer": answer,
                "sources": sources,
                "sources_count": len(sources)
            }
        except Exception as e:
            return f"Error In The Question: {e}"

def load_arabic_embedding_model(config):
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=config.EMBEDDING_MODEL,
            model_kwargs={'device': config.DEVICE},
            encode_kwargs={'normalize_embeddings': True}
        )
        return embeddings
    except Exception as e:
        st.error(f"Error In Loading The Embedding Model: {e}")
        return None

def load_arabic_llm(config):
    try:
        tokenizer = AutoTokenizer.from_pretrained(config.LLM_MODEL)
        model = AutoModelForCausalLM.from_pretrained(config.LLM_MODEL)

        text_generation_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=config.MAX_LENGTH,
            temperature=config.TEMPERATURE,
            do_sample=True,
            return_full_text=False
        )

        llm = HuggingFacePipeline(pipeline=text_generation_pipeline)
        return llm
    except Exception as e:
        st.error(f"Error In Loading The LLM: {e}")
        return None

def load_vector_database(config, embeddings):
    try:
        vectorstore = Chroma(
            persist_directory=config.DATABASE_PATH,
            embedding_function=embeddings,
        )
        return vectorstore
    except Exception as e:
        st.error(f"Error in loading your database: {e}")
        return None

def create_rag_chain(llm, vectorstore):
    arabic_prompt_template = """
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ØµÙ…Ù…Ù‡ Ù…Ø­Ù…Ø¯ Ø¹Ø·ÙˆØ§Ù† Ù„Ù…Ø´Ø±ÙˆØ¹ ØªØ®Ø±Ø¬Ù‡ (ØªØ­Ø¯Ø« Ù…Ø¹ ÙƒØªØ§Ø¨Ùƒ) ØªØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø·. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù… ÙÙ‚Ø·.

Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:
- Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø· Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©
- Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ "Ù„Ø§ Ø£Ø¹Ø±Ù" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ø®ØªÙ„Ø§Ù‚ Ø¥Ø¬Ø§Ø¨Ø©
- ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙˆØ§Ø¶Ø­Ø§Ù‹ ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
"""

    PROMPT = PromptTemplate(
        template=arabic_prompt_template,
        input_variables=["context", "question"]
    )

    retriever = vectorstore.as_retriever(
        search_kwargs={"k": RAGConfig.RETRIEVAL_K}
    )

    rag_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

    return rag_chain

# Streamlit UI
def main():
    st.set_page_config(
        page_title="ØªØ­Ø¯Ø« Ù…Ø¹ ÙƒØªØ§Ø¨Ùƒ - Ù…Ø­Ù…Ø¯ Ø¹Ø·ÙˆØ§Ù†",
        page_icon="ğŸ“š",
        layout="wide"
    )

    # Custom CSS
    st.markdown("""
    <style>
    .css-1y4p8pa {
        direction: rtl;
    }
    .stTextInput {
        direction: rtl;
    }
    </style>
    """, unsafe_allow_html=True)

    st.title("ğŸ¤– ØªØ­Ø¯Ø« Ù…Ø¹ ÙƒØªØ§Ø¨Ùƒ")
    st.markdown("##### Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù„Ù„ØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")

    # Initialize session state
    if 'chatbot' not in st.session_state:
        config = RAGConfig()
        embeddings = load_arabic_embedding_model(config)
        llm = load_arabic_llm(config)
        vectorstore = load_vector_database(config, embeddings)
        
        if all([embeddings, llm, vectorstore]):
            rag_chain = create_rag_chain(llm, vectorstore)
            st.session_state.chatbot = ArabicChatBot(rag_chain, vectorstore)
            st.success("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø¬Ø§Ø­")
        else:
            st.error("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…")
            return

    # Chat interface
    with st.form("chat_form"):
        user_question = st.text_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§:", placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø±Ø£Ø© ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ØŸ")
        submit_button = st.form_submit_button("Ø¥Ø±Ø³Ø§Ù„")

    if submit_button and user_question:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ..."):
            response = st.session_state.chatbot.ask_question(user_question)

            if isinstance(response, dict):
                # Display answer
                st.markdown("### Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
                st.write(response["answer"])

                # Display sources in an expander
                with st.expander(f"ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± (ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… {response['sources_count']} Ù…ØµØ¯Ø±)"):
                    for i, source in enumerate(response["sources"], 1):
                        st.markdown(f"**Ø§Ù„Ù…ØµØ¯Ø± {i}:**")
                        st.markdown(f"```\n{source.page_content[:300]}...\n```")
                        st.markdown("**Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©:**")
                        st.json(source.metadata)
            else:
                st.error(response)

    # Add footer
    st.markdown("---")
    st.markdown("ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨ÙˆØ§Ø³Ø·Ø© Ù…Ø­Ù…Ø¯ Ø¹Ø·ÙˆØ§Ù† Â© 2025")

if __name__ == "__main__":
    main()
