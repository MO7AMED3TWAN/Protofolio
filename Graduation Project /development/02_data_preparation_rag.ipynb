{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8836a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06324d1c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-08T05:26:12.002339Z",
     "iopub.status.busy": "2025-09-08T05:26:12.002120Z",
     "iopub.status.idle": "2025-09-08T05:26:12.006367Z",
     "shell.execute_reply": "2025-09-08T05:26:12.005581Z",
     "shell.execute_reply.started": "2025-09-08T05:26:12.002314Z"
    },
    "papermill": {
     "duration": 1405.640289,
     "end_time": "2025-06-26T15:09:03.585206",
     "exception": false,
     "start_time": "2025-06-26T14:45:37.944917",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install numpy --break-system-packages -q\n",
    "!pip install chromadb --break-system-packages -q\n",
    "!pip install sentence-transformers --break-system-packages -q\n",
    "!pip install langchain --break-system-packages -q\n",
    "!pip install openai --break-system-packages -q\n",
    "!pip install python-dotenv --break-system-packages -q\n",
    "!pip install tiktoken  --break-system-packages -q\n",
    "!pip install langchain-community --break-system-packages -q\n",
    "!pip install datasets --break-system-packages -q\n",
    "!pip install optimum --break-system-packages -q\n",
    "!pip install bitsandbytes --break-system-packages -q\n",
    "!pip install bert_score --break-system-packages -q\n",
    "!pip install transformers --break-system-packages -q\n",
    "!pip install trl --break-system-packages -q\n",
    "!pip install PyPDF2 --break-system-packages -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb53619",
   "metadata": {
    "papermill": {
     "duration": 0.007936,
     "end_time": "2025-06-26T15:09:03.602189",
     "exception": false,
     "start_time": "2025-06-26T15:09:03.594253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 1 - Importing The Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9c46e",
   "metadata": {
    "papermill": {
     "duration": 39.659517,
     "end_time": "2025-06-26T15:09:43.269854",
     "exception": true,
     "start_time": "2025-06-26T15:09:03.610337",
     "status": "failed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Basic Data Processing\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# PDF Processing\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "\n",
    "# Vector Database\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# LangChain Components\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ad30b",
   "metadata": {},
   "source": [
    "# Step 1 - PDF Processing and JSON Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31252f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_to_json(pdf_path, output_json_path):\n",
    "    \"\"\"\n",
    "    Process PDF file and extract content into JSON format\n",
    "    \"\"\"\n",
    "    # Read PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    \n",
    "    # Initialize list to store all documents\n",
    "    documents = []\n",
    "    \n",
    "    # Process each page\n",
    "    for page_num, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        # Split text into sections (you might need to adjust this based on your PDF structure)\n",
    "        sections = re.split(r'\\n\\s*\\n', text)\n",
    "        \n",
    "        for section in sections:\n",
    "            if len(section.strip()) > 50:  # Minimum content length\n",
    "                # Create document structure\n",
    "                doc = {\n",
    "                    \"Title\": f\"Page {page_num + 1} - Section\",  # You might want to extract actual titles\n",
    "                    \"Context\": section.strip(),\n",
    "                    \"Page\": page_num + 1\n",
    "                }\n",
    "                documents.append(doc)\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(documents, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Processed {len(reader.pages)} pages into {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1734c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 55 pages into 112 documents\n"
     ]
    }
   ],
   "source": [
    "# Process the PDF and save to JSON\n",
    "pdf_path = \"inputs/data/raw/data.pdf\"\n",
    "json_output_path = \"outputs/data/processed/data.json\"\n",
    "\n",
    "documents = process_pdf_to_json(pdf_path, json_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088d265",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Step 2 - Loading and Restructuring JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f1174a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_the_data(file_path):\n",
    "    \"\"\"Load and process the JSON file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        print(f\"Successfully loaded {len(data)} documents\")\n",
    "        return data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found!\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_the_data(data):\n",
    "    \"\"\"Analyze the loaded data\"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"\\nData Analysis:\")\n",
    "    print(f\"Total documents: {len(df)}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Calculate text lengths\n",
    "    df['title_length'] = df['Title'].apply(len)\n",
    "    df['context_length'] = df['Context'].apply(len)\n",
    "    \n",
    "    print(f\"\\nText Length Statistics:\")\n",
    "    print(f\"Average title length: {df['title_length'].mean():.1f} characters\")\n",
    "    print(f\"Average context length: {df['context_length'].mean():.1f} characters\")\n",
    "    print(f\"Total context characters: {df['context_length'].sum():,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_documents(data, max_display=5):\n",
    "    \"\"\"Display the documents in a readable format\"\"\"\n",
    "    print(f\"\\nDocuments (showing first {max_display}):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, doc in enumerate(data[:max_display]):\n",
    "        print(f\"\\n{i+1}. {doc['Title']}\")\n",
    "        print(f\"   {doc['Context'][:100]}...\")\n",
    "        print(f\"   Length: {len(doc['Context'])} characters\")\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e8c686-c609-4927-a36a-6a8071162f89",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 112 documents\n"
     ]
    }
   ],
   "source": [
    "file_path = \"outputs/data/processed/data.json\"\n",
    "    \n",
    "# Load the data\n",
    "whole_data = load_the_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b536495-54ed-4f7e-9659-cadd6c4d38aa",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents (showing first 5):\n",
      "--------------------------------------------------\n",
      "\n",
      "1. Page 1 - Section\n",
      "   الوثيقة القانونية والتشريعات الصادرة لصاحل املرأة الصرية...\n",
      "   Length: 56 characters\n",
      "------------------------------\n",
      "\n",
      "2. Page 2 - Section\n",
      "   2 \n",
      "  النظام القانوينجلهموورية صصر العريية   \n",
      "السياق العام للبلد \n",
      " العاصمة: القاهرة   \n",
      "عدد السكان1 : ...\n",
      "   Length: 1898 characters\n",
      "------------------------------\n",
      "\n",
      "3. Page 2 - Section\n",
      "   1 - الجهاز المركزي للتعبئة العامة واالحصاء؛ الكتاب  االحصائي السنوي، سبتمبر 2015. \n",
      "2 - الجهاز المركز...\n",
      "   Length: 205 characters\n",
      "------------------------------\n",
      "\n",
      "4. Page 3 - Section\n",
      "   3 \n",
      "  الناتج المحلىاإلجمالي4  المصري عن عام 2013/2014  = بسعر السوق 1643,4 .مليار جنية مصري 5 \n",
      " معدل ...\n",
      "   Length: 180 characters\n",
      "------------------------------\n",
      "\n",
      "5. Page 3 - Section\n",
      "   4  الناتج المحلى االجماليبسعر السوق: ما انتجه من سلع وخدمات باألسعار الجارية مستبعد فئة المستلزمات ا...\n",
      "   Length: 428 characters\n",
      "------------------------------\n",
      "\n",
      "Data Analysis:\n",
      "Total documents: 112\n",
      "Columns: ['Title', 'Context', 'Page']\n",
      "\n",
      "Text Length Statistics:\n",
      "Average title length: 16.9 characters\n",
      "Average context length: 1179.1 characters\n",
      "Total context characters: 132,056\n"
     ]
    }
   ],
   "source": [
    "if whole_data:    \n",
    "    # Display documents\n",
    "    display_documents(whole_data)\n",
    "\n",
    "    # Show Some Analysis\n",
    "    analyze_the_data(whole_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff641a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Step 3 - Document Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db01bcb-fdf6-4e2a-b7ba-8ed51f9b20b0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def Create_documents(whole_data):\n",
    "    \"\"\"\n",
    "    Create documents suitable for embedding models\n",
    "    Returns: List of Document objects with combined text for embedding\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for item in whole_data:\n",
    "        if isinstance(item, list):\n",
    "            item = item[0]\n",
    "\n",
    "        title = item.get(\"Title\", \"\")\n",
    "        context = item.get(\"Context\", \"\")\n",
    "\n",
    "        if title and context:\n",
    "            # Combine title and context for better embedding quality\n",
    "            combined_text = f\"{title}\\n\\n{context}\"\n",
    "            \n",
    "            documents.append(Document(\n",
    "                page_content=combined_text,  # This is what gets embedded\n",
    "                metadata={\n",
    "                    \"title\": title,\n",
    "                    \"context\": context,\n",
    "                    \"text_length\": len(combined_text)\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90f157f-3e58-4665-9210-3412fd67e0a1",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Usage:\n",
    "documents = Create_documents(whole_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a6a941c-2619-402e-9db6-156b4b802fdd",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents lenght : 112\n",
      "\n",
      "Document 1:\n",
      "Title : Page 1 - Section\n",
      "\n",
      "الوثيقة القانونية والتشريعات الصادرة لصاحل املرأة الصرية...\n",
      "Meta Data : {'title': 'Page 1 - Section', 'context': 'الوثيقة القانونية والتشريعات الصادرة لصاحل املرأة الصرية', 'text_length': 74}\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "Title : Page 2 - Section\n",
      "\n",
      "2 \n",
      "  النظام القانوينجلهموورية صصر العريية   \n",
      "السياق العام للبلد \n",
      " العاصمة: القاهرة...\n",
      "Meta Data : {'title': 'Page 2 - Section', 'context': '2 \\n  النظام القانوينجلهموورية صصر العريية   \\nالسياق العام للبلد \\n العاصمة: القاهرة   \\nعدد السكان1 : 96,521,618 \\nاللغة الرسمية: العربية \\nالمساحة الجغرافية: تقع جمهورية مصر العربية فى الشمال الشرقي لقار ة \\nإفريقيا، وتطل على كل من الساحل الجنوبي الشرقي للبحر المتوسط \\nوالساحل الشمالي الغربي للبحر االحمر بمساحة اجمالية تبلغ مليون كم 2 \\n تقريباً، مصر دولة إفريقية غير أن جزءا من أراضيها، وهى شبه جزيرة\\nسيناء\" يقع في قارة أسيا. \\nلمصر حدود مشتركة مع ليبيا من الغرب، مع السودان من الجنوب، ومع \\nاسرائيل وفلسطين (قطاع غزة) من الشمال الشرقي، ويمر الممر المائي (قناة \\nالسويس) الذى يربط البحر المتوسط والبحر االحمر عبر االراضي المصرية \\nفاصالً الجزء األفريقي منها وهو األكبر مساحة عن الجزء األسيوي األصغر \\nمنها\"، تشكل الصحراء غالبية مساحة مصر، ويتركز أغلب سكان الجمهورية  \\nفي وادى النيل والدلتا2. \\n كماجاء في دستورها الجديد( 2014 )،جمهورية مصر العربية دولة ذات سيادة، موحدة ال تقبل التجزئة، وال  \\nينزل عن شيء منها، نظامها جمهوري  ديمقراطي، يقوم على أساس المواطنة وسيادة القانون   و يقوم النظام \\nالسياسي على أساس التعددية السياسية والحزبية، والتداول السلمى للسلطة، والفصل بين السلطات والتوازن \\nبينها، رئيس الجمهورية هو رئيس الدولة، ورئيس السلطة التنفيذية، و يكلف رئيساً لمجلس الوزراء، بتشكيل \\nالحكومة وعرض برنامجه على مجلس النواب الذي يتولى سلطة التشريع، وا قرار السياسة العامة للدولة، والخطة \\nالعامة للتنمية االقتصادية، واالجتماعية، والموازنة العامة للدولة، ويمارس الرقابة على أعمال السلطة التنفيذية.3 البنية السياسية \\nبدأ االقتصاد المصري معتمدا على الزراعة بشهرة بعض المحاصيل كالقطن والفواكه ثم تطور الصناعات مع \\nالنظام االشتراكي وتجلت السياحة من أهم مصادر االقتصاد  زيادة على تعدد الثروات الطبيعية مثل النفط \\nوالغاز الطبيعي وعائدات قناة السويس كممر مالحي عالمي بجانب نشاطات اإلعالم و في الفترة األخيرة، اتجه \\nاالقتصاد نحو االقتصاد الحر وزيادة االستثمارات . يعتبر االقتصاد المصري  الثاني في الدول العربية بعد \\nالمملكة العربية السعودية والثاني إفريقيا بعد جنوب إفريقيا. \\nالعملة: الجنيه المصري   النظام االقتصادي', 'text_length': 1916}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Documents lenght : {len(documents)}\")\n",
    "\n",
    "for i, doc in enumerate(documents[:2]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Title : {doc.page_content[:100]}...\")\n",
    "    print(f\"Meta Data : {doc.metadata}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ca63793-8d5d-4c4e-8569-d62bb7b8d9f1",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 112 documents for embedding\n",
      "Sample text for embedding: Page 2 - Section\n",
      "\n",
      "2 \n",
      "  النظام القانوينجلهموورية صصر العريية   \n",
      "السياق العام للبلد \n",
      " العاصمة: القاهرة   \n",
      "عدد السكان1 : 96,521,618 \n",
      "اللغة الرسمية: العربية \n",
      "المساحة الجغرافية: تقع جمهورية مصر العربية فى الشمال الشرقي لقار ة \n",
      "إفريقيا، وتطل على كل من الساحل الجنوبي الشرقي للبحر المتوسط \n",
      "والساحل الشمالي الغربي للبحر االحمر بمساحة اجمالية تبلغ مليون كم 2 \n",
      " تقريباً، مصر دولة إفريقية غير أن جزءا من أراضيها، وهى شبه جزيرة\n",
      "سيناء\" يقع في قارة أسيا. \n",
      "لمصر حدود مشتركة مع ليبيا من الغرب، مع السودان من الجنوب، ومع \n",
      "اسرائيل وفلسطين (قطاع غزة) من الشمال الشرقي، ويمر الممر المائي (قناة \n",
      "السويس) الذى يربط البحر المتوسط والبحر االحمر عبر االراضي المصرية \n",
      "فاصالً الجزء األفريقي منها وهو األكبر مساحة عن الجزء األسيوي األصغر \n",
      "منها\"، تشكل الصحراء غالبية مساحة مصر، ويتركز أغلب سكان الجمهورية  \n",
      "في وادى النيل والدلتا2. \n",
      " كماجاء في دستورها الجديد( 2014 )،جمهورية مصر العربية دولة ذات سيادة، موحدة ال تقبل التجزئة، وال  \n",
      "ينزل عن شيء منها، نظامها جمهوري  ديمقراطي، يقوم على أساس المواطنة وسيادة القانون   و يقوم النظام \n",
      "السياسي على أساس التعددية السياسية والحزبية، والتداول السلمى للسلطة، والفصل بين السلطات والتوازن \n",
      "بينها، رئيس الجمهورية هو رئيس الدولة، ورئيس السلطة التنفيذية، و يكلف رئيساً لمجلس الوزراء، بتشكيل \n",
      "الحكومة وعرض برنامجه على مجلس النواب الذي يتولى سلطة التشريع، وا قرار السياسة العامة للدولة، والخطة \n",
      "العامة للتنمية االقتصادية، واالجتماعية، والموازنة العامة للدولة، ويمارس الرقابة على أعمال السلطة التنفيذية.3 البنية السياسية \n",
      "بدأ االقتصاد المصري معتمدا على الزراعة بشهرة بعض المحاصيل كالقطن والفواكه ثم تطور الصناعات مع \n",
      "النظام االشتراكي وتجلت السياحة من أهم مصادر االقتصاد  زيادة على تعدد الثروات الطبيعية مثل النفط \n",
      "والغاز الطبيعي وعائدات قناة السويس كممر مالحي عالمي بجانب نشاطات اإلعالم و في الفترة األخيرة، اتجه \n",
      "االقتصاد نحو االقتصاد الحر وزيادة االستثمارات . يعتبر االقتصاد المصري  الثاني في الدول العربية بعد \n",
      "المملكة العربية السعودية والثاني إفريقيا بعد جنوب إفريقيا. \n",
      "العملة: الجنيه المصري   النظام االقتصادي...\n"
     ]
    }
   ],
   "source": [
    "# For embeddings, you'll use:\n",
    "texts = [doc.page_content for doc in documents] \n",
    "\n",
    "print(f\"Created {len(documents)} documents for embedding\")\n",
    "print(f\"Sample text for embedding: {texts[1][:]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a731ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Step 5 - Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaa3f91b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "dim = 768\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08608412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras --break-system-packages -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99b4d7a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 15:08:05.850929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 15:08:08.288742: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/activations_tf.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_id, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m SentenceTransformerEmbeddings(\n\u001b[1;32m      6\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m      7\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device}\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sentence_transformers/__init__.py:15\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[1;32m     12\u001b[0m     export_optimized_onnx_model,\n\u001b[1;32m     13\u001b[0m     export_static_quantized_openvino_model,\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     CrossEncoder,\n\u001b[1;32m     17\u001b[0m     CrossEncoderModelCardData,\n\u001b[1;32m     18\u001b[0m     CrossEncoderTrainer,\n\u001b[1;32m     19\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfit_mixin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FitMixin\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData, generate_model_card\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     cross_encoder_init_args_decorator,\n\u001b[1;32m     33\u001b[0m     cross_encoder_predict_rank_args_decorator,\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sentence_transformers/cross_encoder/fit_mixin.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sentence_transformers/datasets/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module, get_relative_import_files\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Router\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mModule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sentence_transformers/model_card.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeCarbonCallback\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_markdown_table\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerControl, TrainerState\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2302\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[1;32m   2301\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2302\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2303\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2304\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2332\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2330\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2329\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2330\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2331\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:60\u001b[0m\n\u001b[1;32m     57\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFPreTrainedModel\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2302\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[1;32m   2301\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2302\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2303\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2304\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2332\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2330\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2329\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2330\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2331\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/activations_tf.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(model_id, device=device)\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=model_id,\n",
    "    model_kwargs={'device': device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f0622",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.util import batch_to_device\n",
    "  \n",
    "def batch_encode(texts, model, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        emb = model.encode(batch, show_progress_bar=False)\n",
    "        embeddings.extend(emb)\n",
    "    return embeddings\n",
    " \n",
    "encoded_docs = batch_encode(list(doc_texts.values()), model, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d1f43",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Data_docs = [doc for doc in documents]\n",
    "\n",
    "Data_texts = []\n",
    "\n",
    "for doc in Data_docs:\n",
    "    Title = doc.metadata.get(\"title\")\n",
    "    Context = doc.metadata.get(\"context\")\n",
    "\n",
    "encoded_data = batch_encode(Data_texts, model, batch_size=16)\n",
    "\n",
    "encoded_data = [e.tolist() for e in encoded_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9f208",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Step 6 - Create The Vector Database - ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9a60a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./outputs/data/vector_database/vector_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4d4d8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(\n",
    "    name=\"ar_docs\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527441c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=doc_text,\n",
    "    embeddings=embedding_questions,\n",
    "    metadatas=QA_metadata,\n",
    "    ids=ids_to_add\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a491c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=\"ar_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1ef9f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    collection_name=\"ar_docs\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./outputs/vector_database/vector_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe30e24",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "question = \"ما هي مظاهر الحياة السياسية بالدولة المصرية ؟\"\n",
    "\n",
    "\n",
    "question_embed = model.encode([question])[0].tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[question_embed],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21fd9a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if results[\"documents\"]:\n",
    "    top_match_metadata = results[\"metadatas\"][0][0]\n",
    "    Matched_Title = top_match_metadata.get(\"Title\", \"\")\n",
    "    Context = top_match_metadata.get(\"Context\")\n",
    "\n",
    "    print(\"Nearest Matched Titles: \", Matched_Title)\n",
    "    print(\"Context Of The Nearest Titles:\", Context)\n",
    "    \n",
    "else:\n",
    "    print(\"No Find Any !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab51cf5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Step 7 - Evaluation The embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15623830",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040871a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chroma_results = []\n",
    "qa_embeddings = []  \n",
    "\n",
    "for embedding in encoded_questions:\n",
    "    results = vectordb.similarity_search_by_vector(\n",
    "        embedding=embedding, \n",
    "        k=3\n",
    "    )\n",
    "    chroma_results.append(results)\n",
    "    qa_embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d5fad0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14176bc1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chroma_insights = {\n",
    "    \"valid\": 0,\n",
    "    \"similar\": 0,\n",
    "    \"invalid\": 0\n",
    "}\n",
    "\n",
    "for i in range(len(qa_texts)):\n",
    "    true_metadata = QA_metadata[i]  \n",
    "\n",
    "    pred_metadata = chroma_results[i][0].metadata\n",
    "\n",
    "    true_id = str(true_metadata.get(\"id\", \"\"))\n",
    "    pred_id = str(pred_metadata.get(\"id\", \"\"))\n",
    "\n",
    "    true_source = true_metadata.get(\"source\", \"\")\n",
    "    pred_source = pred_metadata.get(\"source\", \"\")\n",
    "\n",
    "    # تصنيف النتائج\n",
    "    if true_id == pred_id:\n",
    "        chroma_insights[\"valid\"] += 1\n",
    "    elif true_source == pred_source:\n",
    "        chroma_insights[\"similar\"] += 1\n",
    "    else:\n",
    "        chroma_insights[\"invalid\"] += 1\n",
    "\n",
    "# حساب النسب\n",
    "total = len(qa_texts)\n",
    "chroma_insights[\"valid_percentage\"] = chroma_insights[\"valid\"] / total\n",
    "chroma_insights[\"similar_percentage\"] = chroma_insights[\"similar\"] / total\n",
    "chroma_insights[\"invalid_percentage\"] = chroma_insights[\"invalid\"] / total\n",
    "\n",
    "# طباعة النتائج\n",
    "print(\"Model ID:\", model_id)\n",
    "print(\"----\")\n",
    "print(\"Valid:\", chroma_insights[\"valid\"])\n",
    "print(\"Valid%:\", chroma_insights[\"valid_percentage\"])\n",
    "print(\"----\")\n",
    "print(\"Similar:\", chroma_insights[\"similar\"])\n",
    "print(\"Similar%:\", chroma_insights[\"similar_percentage\"])\n",
    "print(\"----\")\n",
    "print(\"Invalid:\", chroma_insights[\"invalid\"])\n",
    "print(\"Invalid%:\", chroma_insights[\"invalid_percentage\"])\n",
    "print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8214417,
     "sourceId": 12978273,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1453.019756,
   "end_time": "2025-06-26T15:09:46.241755",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-26T14:45:33.221999",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
