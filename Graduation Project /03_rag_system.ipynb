{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main RAG System - Speak with Your Book\n",
    "# This notebook loads the pre-built model and database to create an interactive chat system\n",
    "\n",
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ebf01",
   "metadata": {},
   "source": [
    "# 1. CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGConfig:\n",
    "    \"\"\"Configuration for the RAG System\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    MODEL_PATH = \"development/outputs/fine_tuned_model\"\n",
    "    DATABASE_PATH = \"development/outputs/vector_database\"\n",
    "    \n",
    "    # Model settings\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    MAX_LENGTH = 512\n",
    "    TEMPERATURE = 0.7\n",
    "    \n",
    "    # Retrieval settings\n",
    "    RETRIEVAL_K = 5\n",
    "    CHUNK_SIZE = 500\n",
    "    \n",
    "    # Embedding model\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6812fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RAGConfig()\n",
    "print(f\"Initializing RAG System...\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "print(f\"Model Path: {config.MODEL_PATH}\")\n",
    "print(f\"Database Path: {config.DATABASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b8d45",
   "metadata": {},
   "source": [
    "# 2. LOAD FINE-TUNED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_PATH)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.MODEL_PATH,\n",
    "        torch_dtype=torch.float16 if config.DEVICE == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if config.DEVICE == \"cuda\" else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Create HuggingFace pipeline\n",
    "    from transformers import pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=config.MAX_LENGTH,\n",
    "        temperature=config.TEMPERATURE,\n",
    "        do_sample=True,\n",
    "        device=0 if config.DEVICE == \"cuda\" else -1\n",
    "    )\n",
    "    \n",
    "    # Wrap in LangChain\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Falling back to default model...\")\n",
    "    # Fallback to base model\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "        model_id=\"Qwen/Qwen-1_8B-Chat\",\n",
    "        task=\"text-generation\",\n",
    "        model_kwargs={\"temperature\": config.TEMPERATURE, \"max_length\": config.MAX_LENGTH}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdefec06",
   "metadata": {},
   "source": [
    "# 3. LOAD VECTOR DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b25e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=config.EMBEDDING_MODEL,\n",
    "        model_kwargs={'device': config.DEVICE}\n",
    "    )\n",
    "    \n",
    "    # Load ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=config.DATABASE_PATH)\n",
    "    vectorstore = Chroma(\n",
    "        client=chroma_client,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"book_collection\"\n",
    "    )\n",
    "    \n",
    "    # Test database\n",
    "    collection_count = vectorstore._collection.count()\n",
    "    print(f\"✅ Database loaded successfully!\")\n",
    "    print(f\"📚 Total documents in database: {collection_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading database: {e}\")\n",
    "    print(\"💡 Make sure the database was created properly in development phase\")\n",
    "    vectorstore = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf553a",
   "metadata": {},
   "source": [
    "# 4. CREATE RAG CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompt template\n",
    "prompt_template = \"\"\"\n",
    "أنت مساعد شخصي يمكنكك من التحدث عن الكتب التي قرأتها والرد على اسئلة المستخدمين حول محتوى الكتب باللغة العربية الفصحي\n",
    "المحتوي من الكتاب:\n",
    "{context}\n",
    "\n",
    "السؤال : {question}\n",
    "\n",
    "التعليمات: \n",
    "- الإجابة بناءً على السياق المقدم\n",
    "- إذا كانت الإجابة غير موجودة في السياق، قل \"لا أستطيع العثور على هذه المعلومات في الكتاب\"\n",
    "- كن محددًا واستشهد بالتفاصيل ذات الصلة\n",
    "- اجعل إجابتك مركزة وذات صلة\n",
    "\n",
    "الإجابة:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create retrieval chain\n",
    "if vectorstore is not None:\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": config.RETRIEVAL_K}\n",
    "    )\n",
    "    \n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"RAG Chain created successfully!\")\n",
    "else:\n",
    "    rag_chain = None\n",
    "    print(\"Cannot create RAG chain without database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f385cb6",
   "metadata": {},
   "source": [
    "# 5. CHAT INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854fbcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookChatbot:\n",
    "    \"\"\"Interactive chatbot for book conversations\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_chain, vectorstore):\n",
    "        self.rag_chain = rag_chain\n",
    "        self.vectorstore = vectorstore\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def ask_question(self, question: str):\n",
    "        \"\"\"Ask a question and get an answer\"\"\"\n",
    "        if not self.rag_chain:\n",
    "            return \"RAG system not properly initialized\"\n",
    "        \n",
    "        try:\n",
    "            # Get answer from RAG chain\n",
    "            response = self.rag_chain({\"query\": question})\n",
    "            \n",
    "            # Extract answer and sources\n",
    "            answer = response[\"result\"]\n",
    "            sources = response[\"source_documents\"]\n",
    "            \n",
    "            # Store in history\n",
    "            self.chat_history.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"sources_count\": len(sources)\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources,\n",
    "                \"sources_count\": len(sources)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error processing question: {e}\"\n",
    "    \n",
    "    def get_relevant_chunks(self, question: str, k: int = 3):\n",
    "        \"\"\"Get relevant document chunks for debugging\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "        \n",
    "        docs = self.vectorstore.similarity_search(question, k=k)\n",
    "        return [{\"content\": doc.page_content, \"metadata\": doc.metadata} for doc in docs]\n",
    "    \n",
    "    def show_stats(self):\n",
    "        \"\"\"Show system statistics\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return \"Database not loaded\"\n",
    "        \n",
    "        collection_count = self.vectorstore._collection.count()\n",
    "        chat_count = len(self.chat_history)\n",
    "        \n",
    "        return f\"\"\"\n",
    "        RAG System Statistics:\n",
    "        ━━━━━━━━━━━━━━━━━━━━━━━\n",
    "        Documents in database: {collection_count}\n",
    "        Questions asked: {chat_count}\n",
    "        Model: Fine-tuned Qwen\n",
    "        Retrieval method: Semantic similarity\n",
    "        Max retrieval chunks: {config.RETRIEVAL_K}\n",
    "        \"\"\"\n",
    "\n",
    "# Initialize chatbot\n",
    "if rag_chain:\n",
    "    chatbot = BookChatbot(rag_chain, vectorstore)\n",
    "    print(\"Chatbot ready!\")\n",
    "else:\n",
    "    chatbot = None\n",
    "    print(\"Chatbot initialization failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdaaf77",
   "metadata": {},
   "source": [
    "# 6. TESTING & DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641def08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Speck With Your Book\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if chatbot:\n",
    "    print(chatbot.show_stats())\n",
    "    \n",
    "    print(\"\\n💡 Example Usage:\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Example questions for testing\n",
    "    example_questions = [\n",
    "        \"What is the main theme of this book?\",\n",
    "        \"Who are the main characters?\",\n",
    "        \"What happens in the first chapter?\",\n",
    "        \"Summarize the conclusion\",\n",
    "        \"What is the author's main argument?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Try asking questions like:\")\n",
    "    for i, q in enumerate(example_questions, 1):\n",
    "        print(f\"   {i}. {q}\")\n",
    "    \n",
    "    print(\"\\nStart asking questions:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Interactive chat loop\n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\nYour question (or 'quit' to exit): \").strip()\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"Thanks for using the RAG system!\")\n",
    "                break\n",
    "            \n",
    "            if not question:\n",
    "                continue\n",
    "            \n",
    "            print(\"Thinking...\")\n",
    "            response = chatbot.ask_question(question)\n",
    "            \n",
    "            if isinstance(response, dict):\n",
    "                print(f\"\\nAnswer:\")\n",
    "                print(\"-\" * 20)\n",
    "                print(response[\"answer\"])\n",
    "                print(f\"\\nRetrieved {response['sources_count']} relevant chunks\")\n",
    "                \n",
    "                # Show sources if requested\n",
    "                show_sources = input(\"\\n🔍 Show source chunks? (y/n): \").lower()\n",
    "                if show_sources == 'y':\n",
    "                    print(\"\\nSource chunks:\")\n",
    "                    print(\"-\" * 30)\n",
    "                    for i, source in enumerate(response[\"sources\"][:3], 1):\n",
    "                        print(f\"\\nChunk {i}:\")\n",
    "                        print(f\"Content: {source.page_content[:200]}...\")\n",
    "                        print(f\"Metadata: {source.metadata}\")\n",
    "            else:\n",
    "                print(f\"{response}\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n👋 Session ended by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"System not ready. Please check model and database setup.\")\n",
    "    print(\"\\n Troubleshooting:\")\n",
    "    print(\"1. Make sure development/outputs/fine_tuned_model exists\")\n",
    "    print(\"2. Make sure development/outputs/vector_database exists\")\n",
    "    print(\"3. Run the development notebooks first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
